{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74ffd477",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T08:42:56.556191Z",
     "iopub.status.busy": "2025-12-02T08:42:56.555867Z",
     "iopub.status.idle": "2025-12-02T08:43:06.082980Z",
     "shell.execute_reply": "2025-12-02T08:43:06.081448Z"
    },
    "papermill": {
     "duration": 9.536522,
     "end_time": "2025-12-02T08:43:06.085851",
     "exception": false,
     "start_time": "2025-12-02T08:42:56.549329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/02 08:43:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"stroke-prediction\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90d55984",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T08:43:06.098780Z",
     "iopub.status.busy": "2025-12-02T08:43:06.098153Z",
     "iopub.status.idle": "2025-12-02T08:43:15.483680Z",
     "shell.execute_reply": "2025-12-02T08:43:15.481762Z"
    },
    "papermill": {
     "duration": 9.394662,
     "end_time": "2025-12-02T08:43:15.486004",
     "exception": false,
     "start_time": "2025-12-02T08:43:06.091342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = '/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv'\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90710098",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T08:43:15.507989Z",
     "iopub.status.busy": "2025-12-02T08:43:15.506821Z",
     "iopub.status.idle": "2025-12-02T08:43:16.168705Z",
     "shell.execute_reply": "2025-12-02T08:43:16.167748Z"
    },
    "papermill": {
     "duration": 0.674559,
     "end_time": "2025-12-02T08:43:16.170880",
     "exception": false,
     "start_time": "2025-12-02T08:43:15.496321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+------+\n",
      "|id   |gender|age |hypertension|heart_disease|ever_married|work_type    |Residence_type|avg_glucose_level|bmi |smoking_status |stroke|\n",
      "+-----+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+------+\n",
      "|9046 |Male  |67.0|0           |1            |Yes         |Private      |Urban         |228.69           |36.6|formerly smoked|1     |\n",
      "|51676|Female|61.0|0           |0            |Yes         |Self-employed|Rural         |202.21           |N/A |never smoked   |1     |\n",
      "|31112|Male  |80.0|0           |1            |Yes         |Private      |Rural         |105.92           |32.5|never smoked   |1     |\n",
      "|60182|Female|49.0|0           |0            |Yes         |Private      |Urban         |171.23           |34.4|smokes         |1     |\n",
      "|1665 |Female|79.0|1           |0            |Yes         |Self-employed|Rural         |174.12           |24  |never smoked   |1     |\n",
      "+-----+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5c24bf",
   "metadata": {
    "papermill": {
     "duration": 0.007244,
     "end_time": "2025-12-02T08:43:16.185020",
     "exception": false,
     "start_time": "2025-12-02T08:43:16.177776",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6339281",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T08:43:16.203118Z",
     "iopub.status.busy": "2025-12-02T08:43:16.202674Z",
     "iopub.status.idle": "2025-12-02T08:43:16.213449Z",
     "shell.execute_reply": "2025-12-02T08:43:16.212519Z"
    },
    "papermill": {
     "duration": 0.022878,
     "end_time": "2025-12-02T08:43:16.215996",
     "exception": false,
     "start_time": "2025-12-02T08:43:16.193118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- hypertension: integer (nullable = true)\n",
      " |-- heart_disease: integer (nullable = true)\n",
      " |-- ever_married: string (nullable = true)\n",
      " |-- work_type: string (nullable = true)\n",
      " |-- Residence_type: string (nullable = true)\n",
      " |-- avg_glucose_level: double (nullable = true)\n",
      " |-- bmi: string (nullable = true)\n",
      " |-- smoking_status: string (nullable = true)\n",
      " |-- stroke: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e31587b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T08:43:16.237372Z",
     "iopub.status.busy": "2025-12-02T08:43:16.236928Z",
     "iopub.status.idle": "2025-12-02T08:43:17.287487Z",
     "shell.execute_reply": "2025-12-02T08:43:17.286506Z"
    },
    "papermill": {
     "duration": 1.063835,
     "end_time": "2025-12-02T08:43:17.289917",
     "exception": false,
     "start_time": "2025-12-02T08:43:16.226082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 5110\n",
      "Total columns: 12\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total rows: {df.count()}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4de6b38b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T08:43:17.310478Z",
     "iopub.status.busy": "2025-12-02T08:43:17.310064Z",
     "iopub.status.idle": "2025-12-02T08:43:18.341934Z",
     "shell.execute_reply": "2025-12-02T08:43:18.340842Z"
    },
    "papermill": {
     "duration": 1.044068,
     "end_time": "2025-12-02T08:43:18.344892",
     "exception": false,
     "start_time": "2025-12-02T08:43:17.300824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|stroke|count|\n",
      "+------+-----+\n",
      "|     1|  249|\n",
      "|     0| 4861|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check class distribution\n",
    "df.groupBy('stroke').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "457cf26f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T08:43:18.358927Z",
     "iopub.status.busy": "2025-12-02T08:43:18.358632Z",
     "iopub.status.idle": "2025-12-02T08:43:19.834070Z",
     "shell.execute_reply": "2025-12-02T08:43:19.832507Z"
    },
    "papermill": {
     "duration": 1.484542,
     "end_time": "2025-12-02T08:43:19.836590",
     "exception": false,
     "start_time": "2025-12-02T08:43:18.352048",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------------+-------------+------------+---------+--------------+-----------------+---+--------------+------+\n",
      "| id|gender|age|hypertension|heart_disease|ever_married|work_type|Residence_type|avg_glucose_level|bmi|smoking_status|stroke|\n",
      "+---+------+---+------------+-------------+------------+---------+--------------+-----------------+---+--------------+------+\n",
      "|  0|     0|  0|           0|            0|           0|        0|             0|                0|  0|             0|     0|\n",
      "+---+------+---+------------+-------------+------------+---------+--------------+-----------------+---+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "from pyspark.sql.functions import col, sum as spark_sum, isnan, when, count, mean\n",
    "\n",
    "df.select([spark_sum(when(col(c).isNull() | isnan(c), 1).otherwise(0)).alias(c) \n",
    "           for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab89d05",
   "metadata": {
    "papermill": {
     "duration": 0.004368,
     "end_time": "2025-12-02T08:43:19.846077",
     "exception": false,
     "start_time": "2025-12-02T08:43:19.841709",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "993a9388",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T08:43:19.856608Z",
     "iopub.status.busy": "2025-12-02T08:43:19.856034Z",
     "iopub.status.idle": "2025-12-02T08:43:19.875204Z",
     "shell.execute_reply": "2025-12-02T08:43:19.873992Z"
    },
    "papermill": {
     "duration": 0.026729,
     "end_time": "2025-12-02T08:43:19.877086",
     "exception": false,
     "start_time": "2025-12-02T08:43:19.850357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.drop('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6977bc0a",
   "metadata": {
    "papermill": {
     "duration": 0.004417,
     "end_time": "2025-12-02T08:43:19.886493",
     "exception": false,
     "start_time": "2025-12-02T08:43:19.882076",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Feature Engineering & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b89a2fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T08:43:19.897197Z",
     "iopub.status.busy": "2025-12-02T08:43:19.896750Z",
     "iopub.status.idle": "2025-12-02T08:43:21.062689Z",
     "shell.execute_reply": "2025-12-02T08:43:21.061744Z"
    },
    "papermill": {
     "duration": 1.173801,
     "end_time": "2025-12-02T08:43:21.064693",
     "exception": false,
     "start_time": "2025-12-02T08:43:19.890892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- hypertension: integer (nullable = true)\n",
      " |-- heart_disease: integer (nullable = true)\n",
      " |-- ever_married: string (nullable = true)\n",
      " |-- work_type: string (nullable = true)\n",
      " |-- Residence_type: string (nullable = true)\n",
      " |-- avg_glucose_level: double (nullable = true)\n",
      " |-- bmi: double (nullable = false)\n",
      " |-- smoking_status: string (nullable = true)\n",
      " |-- stroke: integer (nullable = true)\n",
      "\n",
      "+------+----+------------+-------------+------------+-------------+--------------+-----------------+------------------+---------------+------+\n",
      "|gender| age|hypertension|heart_disease|ever_married|    work_type|Residence_type|avg_glucose_level|               bmi| smoking_status|stroke|\n",
      "+------+----+------------+-------------+------------+-------------+--------------+-----------------+------------------+---------------+------+\n",
      "|  Male|67.0|           0|            1|         Yes|      Private|         Urban|           228.69|              36.6|formerly smoked|     1|\n",
      "|Female|61.0|           0|            0|         Yes|Self-employed|         Rural|           202.21|28.893236911794673|   never smoked|     1|\n",
      "|  Male|80.0|           0|            1|         Yes|      Private|         Rural|           105.92|              32.5|   never smoked|     1|\n",
      "|Female|49.0|           0|            0|         Yes|      Private|         Urban|           171.23|              34.4|         smokes|     1|\n",
      "|Female|79.0|           1|            0|         Yes|Self-employed|         Rural|           174.12|              24.0|   never smoked|     1|\n",
      "+------+----+------------+-------------+------------+-------------+--------------+-----------------+------------------+---------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, mean, when\n",
    "\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(data_path)\n",
    "\n",
    "# Drop id column\n",
    "df = df.drop('id')\n",
    "\n",
    "# Convert bmi to double and handle nulls\n",
    "df = df.withColumn('bmi', col('bmi').cast('double'))\n",
    "\n",
    "# Fill nulls with mean\n",
    "mean_bmi = df.select(mean('bmi')).collect()[0][0]\n",
    "df = df.fillna({'bmi': mean_bmi})\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eae5b771",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T08:43:21.082102Z",
     "iopub.status.busy": "2025-12-02T08:43:21.081684Z",
     "iopub.status.idle": "2025-12-02T08:43:26.434039Z",
     "shell.execute_reply": "2025-12-02T08:43:26.432636Z"
    },
    "papermill": {
     "duration": 5.363695,
     "end_time": "2025-12-02T08:43:26.436302",
     "exception": false,
     "start_time": "2025-12-02T08:43:21.072607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------+------+\n",
      "|features                                                                                                                          |stroke|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------+------+\n",
      "|[2.9629437376526147,0.0,4.4235458933104335,5.050177133589643,4.7544706732567965,2.0282180648496397,0.0,0.0,0.0,1.8659871947574447]|1     |\n",
      "|(10,[0,3,4,7,8],[2.6976054924896937,4.465417456745646,3.7533346325843864,0.9009626130593035,2.000061825390059])                   |1     |\n",
      "|[3.5378432688389427,0.0,4.4235458933104335,2.3390387073759893,4.22186603499579,2.0282180648496397,0.0,0.0,2.000061825390059,0.0]  |1     |\n",
      "|(10,[0,3,4,9],[2.1669290021638523,3.781283967749156,4.468682818580159,2.798980792136167])                                         |1     |\n",
      "|[3.493620227978456,3.371468300850777,0.0,3.8451040382204233,3.117685687381506,0.0,0.0,0.9009626130593035,2.000061825390059,0.0]   |1     |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Convert categorical columns to numeric indices\n",
    "categorical_cols = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']\n",
    "\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\", handleInvalid=\"keep\") \n",
    "            for col in categorical_cols]\n",
    "\n",
    "# Assemble all features into a vector\n",
    "numerical_cols = ['age', 'hypertension', 'heart_disease', 'avg_glucose_level', 'bmi']\n",
    "indexed_cols = [col+\"_index\" for col in categorical_cols]\n",
    "all_feature_cols = numerical_cols + indexed_cols\n",
    "\n",
    "assembler = VectorAssembler(inputCols=all_feature_cols, outputCol=\"features_raw\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\")\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessing_pipeline = Pipeline(stages=indexers + [assembler, scaler])\n",
    "\n",
    "# Fit and transform\n",
    "pipeline_model = preprocessing_pipeline.fit(df)\n",
    "df_processed = pipeline_model.transform(df).select('features', 'stroke')\n",
    "\n",
    "df_processed.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240fe325",
   "metadata": {
    "papermill": {
     "duration": 0.005647,
     "end_time": "2025-12-02T08:43:26.448518",
     "exception": false,
     "start_time": "2025-12-02T08:43:26.442871",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91bb5b41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T08:43:26.461113Z",
     "iopub.status.busy": "2025-12-02T08:43:26.460296Z",
     "iopub.status.idle": "2025-12-02T08:43:28.315170Z",
     "shell.execute_reply": "2025-12-02T08:43:28.313892Z"
    },
    "papermill": {
     "duration": 1.863253,
     "end_time": "2025-12-02T08:43:28.317134",
     "exception": false,
     "start_time": "2025-12-02T08:43:26.453881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 4123\n",
      "Test set size: 987\n",
      "+------+-----+\n",
      "|stroke|count|\n",
      "+------+-----+\n",
      "|     1|  192|\n",
      "|     0| 3931|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split data (80% train, 20% test)\n",
    "train_df, test_df = df_processed .randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Training set size: {train_df.count()}\")\n",
    "print(f\"Test set size: {test_df.count()}\")\n",
    "\n",
    "train_df.groupBy('stroke').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b475b3",
   "metadata": {
    "papermill": {
     "duration": 0.005114,
     "end_time": "2025-12-02T08:43:28.327145",
     "exception": false,
     "start_time": "2025-12-02T08:43:28.322031",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Handle Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0272c91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T08:43:28.337923Z",
     "iopub.status.busy": "2025-12-02T08:43:28.337608Z",
     "iopub.status.idle": "2025-12-02T08:43:31.172522Z",
     "shell.execute_reply": "2025-12-02T08:43:31.169909Z"
    },
    "papermill": {
     "duration": 2.844127,
     "end_time": "2025-12-02T08:43:31.175949",
     "exception": false,
     "start_time": "2025-12-02T08:43:28.331822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority: 3931, Minority: 192, Ratio: 20.473958333333332\n",
      "Balanced training set size: 7964\n",
      "+------+-----+\n",
      "|stroke|count|\n",
      "+------+-----+\n",
      "|     0| 3931|\n",
      "|     1| 4033|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Separate classes\n",
    "train_majority = train_df.filter(col('stroke') == 0)\n",
    "train_minority = train_df.filter(col('stroke') == 1)\n",
    "\n",
    "# Calculate oversampling ratio\n",
    "majority_count = train_majority.count()\n",
    "minority_count = train_minority.count()\n",
    "ratio = majority_count / minority_count\n",
    "\n",
    "print(f\"Majority: {majority_count}, Minority: {minority_count}, Ratio: {ratio}\")\n",
    "\n",
    "# Oversample minority class\n",
    "train_minority_oversampled = train_minority.sample(withReplacement=True, fraction=ratio, seed=42)\n",
    "\n",
    "# Combine datasets\n",
    "train_balanced = train_majority.union(train_minority_oversampled)\n",
    "\n",
    "print(f\"Balanced training set size: {train_balanced.count()}\")\n",
    "train_balanced.groupBy('stroke').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229b487f",
   "metadata": {
    "papermill": {
     "duration": 0.005644,
     "end_time": "2025-12-02T08:43:31.187596",
     "exception": false,
     "start_time": "2025-12-02T08:43:31.181952",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Train ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb928735",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T08:43:31.201233Z",
     "iopub.status.busy": "2025-12-02T08:43:31.200913Z",
     "iopub.status.idle": "2025-12-02T08:43:50.721945Z",
     "shell.execute_reply": "2025-12-02T08:43:50.720825Z"
    },
    "papermill": {
     "duration": 19.529867,
     "end_time": "2025-12-02T08:43:50.724193",
     "exception": false,
     "start_time": "2025-12-02T08:43:31.194326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models trained successfully!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "# 1. Logistic Regression\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='stroke', maxIter=10)\n",
    "lr_model = lr.fit(train_balanced)\n",
    "\n",
    "# 2. Random Forest\n",
    "rf = RandomForestClassifier(featuresCol='features', labelCol='stroke', numTrees=100, seed=42)\n",
    "rf_model = rf.fit(train_balanced)\n",
    "\n",
    "# 3. Gradient Boosted Trees\n",
    "gbt = GBTClassifier(featuresCol='features', labelCol='stroke', maxIter=10, seed=42)\n",
    "gbt_model = gbt.fit(train_balanced)\n",
    "\n",
    "print(\"Models trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1bdf9a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T08:43:50.736442Z",
     "iopub.status.busy": "2025-12-02T08:43:50.736100Z",
     "iopub.status.idle": "2025-12-02T08:43:53.801022Z",
     "shell.execute_reply": "2025-12-02T08:43:53.799598Z"
    },
    "papermill": {
     "duration": 3.073307,
     "end_time": "2025-12-02T08:43:53.802883",
     "exception": false,
     "start_time": "2025-12-02T08:43:50.729576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression:\n",
      "  AUC: 0.8592\n",
      "  Accuracy: 0.7244\n",
      "\n",
      "Random Forest:\n",
      "  AUC: 0.8474\n",
      "  Accuracy: 0.7204\n",
      "\n",
      "Gradient Boosted Trees:\n",
      "  AUC: 0.8368\n",
      "  Accuracy: 0.7548\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test set\n",
    "lr_predictions = lr_model.transform(test_df)\n",
    "rf_predictions = rf_model.transform(test_df)\n",
    "gbt_predictions = gbt_model.transform(test_df)\n",
    "\n",
    "# Evaluators\n",
    "auc_evaluator = BinaryClassificationEvaluator(labelCol='stroke', metricName='areaUnderROC')\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol='stroke', metricName='accuracy')\n",
    "\n",
    "# Evaluate models\n",
    "models = {\n",
    "    'Logistic Regression': lr_predictions,\n",
    "    'Random Forest': rf_predictions,\n",
    "    'Gradient Boosted Trees': gbt_predictions\n",
    "}\n",
    "\n",
    "for name, predictions in models.items():\n",
    "    auc = auc_evaluator.evaluate(predictions)\n",
    "    accuracy = accuracy_evaluator.evaluate(predictions)\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  AUC: {auc:.4f}\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0aacd990",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T08:43:53.815195Z",
     "iopub.status.busy": "2025-12-02T08:43:53.814891Z",
     "iopub.status.idle": "2025-12-02T08:43:59.380742Z",
     "shell.execute_reply": "2025-12-02T08:43:59.379895Z"
    },
    "papermill": {
     "duration": 5.57395,
     "end_time": "2025-12-02T08:43:59.382327",
     "exception": false,
     "start_time": "2025-12-02T08:43:53.808377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Logistic Regression - Detailed Metrics\n",
      "==================================================\n",
      "\n",
      "Confusion Matrix:\n",
      "[[664. 266.]\n",
      " [  6.  51.]]\n",
      "\n",
      "Class 0 (No Stroke):\n",
      "  Precision: 0.9910\n",
      "  Recall: 0.7140\n",
      "  F1-Score: 0.8300\n",
      "\n",
      "Class 1 (Stroke):\n",
      "  Precision: 0.1609\n",
      "  Recall: 0.8947\n",
      "  F1-Score: 0.2727\n",
      "\n",
      "==================================================\n",
      "Random Forest - Detailed Metrics\n",
      "==================================================\n",
      "\n",
      "Confusion Matrix:\n",
      "[[663. 267.]\n",
      " [  9.  48.]]\n",
      "\n",
      "Class 0 (No Stroke):\n",
      "  Precision: 0.9866\n",
      "  Recall: 0.7129\n",
      "  F1-Score: 0.8277\n",
      "\n",
      "Class 1 (Stroke):\n",
      "  Precision: 0.1524\n",
      "  Recall: 0.8421\n",
      "  F1-Score: 0.2581\n",
      "\n",
      "==================================================\n",
      "Gradient Boosted Trees - Detailed Metrics\n",
      "==================================================\n",
      "\n",
      "Confusion Matrix:\n",
      "[[701. 229.]\n",
      " [ 13.  44.]]\n",
      "\n",
      "Class 0 (No Stroke):\n",
      "  Precision: 0.9818\n",
      "  Recall: 0.7538\n",
      "  F1-Score: 0.8528\n",
      "\n",
      "Class 1 (Stroke):\n",
      "  Precision: 0.1612\n",
      "  Recall: 0.7719\n",
      "  F1-Score: 0.2667\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# Function to calculate detailed metrics\n",
    "def print_detailed_metrics(predictions, model_name):\n",
    "    # Convert to RDD for MulticlassMetrics\n",
    "    predictionAndLabels = predictions.select('prediction', 'stroke').rdd.map(lambda x: (float(x[0]), float(x[1])))\n",
    "    metrics = MulticlassMetrics(predictionAndLabels)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{model_name} - Detailed Metrics\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(metrics.confusionMatrix().toArray())\n",
    "    \n",
    "    # Precision, Recall, F1 for each class\n",
    "    for label in [0.0, 1.0]:\n",
    "        print(f\"\\nClass {int(label)} ({'No Stroke' if label == 0 else 'Stroke'}):\")\n",
    "        print(f\"  Precision: {metrics.precision(label):.4f}\")\n",
    "        print(f\"  Recall: {metrics.recall(label):.4f}\")\n",
    "        print(f\"  F1-Score: {metrics.fMeasure(label):.4f}\")\n",
    "\n",
    "# Print metrics for all models\n",
    "print_detailed_metrics(lr_predictions, \"Logistic Regression\")\n",
    "print_detailed_metrics(rf_predictions, \"Random Forest\")\n",
    "print_detailed_metrics(gbt_predictions, \"Gradient Boosted Trees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20eb5942",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T08:43:59.395771Z",
     "iopub.status.busy": "2025-12-02T08:43:59.395162Z",
     "iopub.status.idle": "2025-12-02T08:44:03.557860Z",
     "shell.execute_reply": "2025-12-02T08:44:03.556267Z"
    },
    "papermill": {
     "duration": 4.172078,
     "end_time": "2025-12-02T08:44:03.560122",
     "exception": false,
     "start_time": "2025-12-02T08:43:59.388044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and pipeline saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the preprocessing pipeline and model\n",
    "pipeline_model.write().overwrite().save(\"/kaggle/working/preprocessing_pipeline\")\n",
    "lr_model.write().overwrite().save(\"/kaggle/working/stroke_prediction_model\")\n",
    "\n",
    "print(\"Model and pipeline saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f849933",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T08:44:03.582374Z",
     "iopub.status.busy": "2025-12-02T08:44:03.581943Z",
     "iopub.status.idle": "2025-12-02T08:44:04.436276Z",
     "shell.execute_reply": "2025-12-02T08:44:04.434788Z"
    },
    "papermill": {
     "duration": 0.867786,
     "end_time": "2025-12-02T08:44:04.438359",
     "exception": false,
     "start_time": "2025-12-02T08:44:03.570573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction: HIGH RISK\n",
      "Stroke Probability: 86.14%\n"
     ]
    }
   ],
   "source": [
    "def predict_stroke(patient_data):\n",
    "    \"\"\"\n",
    "    Predict stroke risk for a new patient\n",
    "    \n",
    "    patient_data: dict with keys: gender, age, hypertension, heart_disease, \n",
    "                  ever_married, work_type, Residence_type, avg_glucose_level, \n",
    "                  bmi, smoking_status\n",
    "    \"\"\"\n",
    "    # Create DataFrame from input\n",
    "    patient_df = spark.createDataFrame([patient_data])\n",
    "    \n",
    "    # Preprocess\n",
    "    patient_processed = pipeline_model.transform(patient_df)\n",
    "    \n",
    "    # Predict\n",
    "    prediction = lr_model.transform(patient_processed)\n",
    "    \n",
    "    result = prediction.select('prediction', 'probability').collect()[0]\n",
    "    \n",
    "    stroke_probability = result['probability'][1]\n",
    "    prediction_label = \"HIGH RISK\" if result['prediction'] == 1 else \"LOW RISK\"\n",
    "    \n",
    "    return {\n",
    "        'prediction': prediction_label,\n",
    "        'stroke_probability': f\"{stroke_probability:.2%}\"\n",
    "    }\n",
    "\n",
    "# Test with a sample patient\n",
    "sample_patient = {\n",
    "    'gender': 'Male',\n",
    "    'age': 67.0,\n",
    "    'hypertension': 1,\n",
    "    'heart_disease': 1,\n",
    "    'ever_married': 'Yes',\n",
    "    'work_type': 'Private',\n",
    "    'Residence_type': 'Urban',\n",
    "    'avg_glucose_level': 228.69,\n",
    "    'bmi': 36.6,\n",
    "    'smoking_status': 'formerly smoked'\n",
    "}\n",
    "\n",
    "result = predict_stroke(sample_patient)\n",
    "print(f\"\\nPrediction: {result['prediction']}\")\n",
    "print(f\"Stroke Probability: {result['stroke_probability']}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1120859,
     "sourceId": 1882037,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 75.94973,
   "end_time": "2025-12-02T08:44:07.064110",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-02T08:42:51.114380",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
